# -*- coding: utf-8 -*-
"""AutoEncoder

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FvaiBdMzLRCR4AhlyrzTtls6ePoegms
"""

import numpy as np
import tensorflow as tf

# Function to define encoder block in my AutoEncoder
def encoder_block( out_channels , kernel_size=(3,3) ,
                 batch_norm=False , activation="relu"):
  assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1 ,"invalied kernal size use (3,3),(5,5) or (7,7) and so on !" # Only odd sizes allowed
  layer=[]
  conv = tf.keras.layers.Conv2D(
    out_channels,kernel_size , padding='same' ,
    activation=None if batch_norm else activation)
  #so i have conv layer now activated if no batch norm and not activated if have batch norm
  layer.append(conv)

  if batch_norm : #if this True this mean i'm not activate , if False this mean i activate and skip .
    ##i apply this mean no activation happen due to True batch norm.
    layer.append(tf.keras.layers.BatchNormalization()) #make batch norm
    #as he may use batch norm and also none for activation , check if we apply activation or not .
    if activation is not None :
      layer.append(tf.keras.layers.Activation(activation))

  return  tf.keras.models.Sequential(layer)

# Function to define decoder block in my AutoEncoder
def decoder_block( out_channels , kernel_size=(3,3) ,
                 batch_norm=False , activation="relu"):
  assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1 ,"invalied kernal size use (3,3),(5,5) or (7,7) and so on !" # Only odd sizes allowed
  layer=[]
  conv = tf.keras.layers.Conv2DTranspose(
    out_channels,kernel_size , padding='same' ,
    activation=None if batch_norm else activation)
  #so i have conv layer now activated if no batch norm and not activated if have batch norm
  layer.append(conv)

  if batch_norm : #if this True this mean i'm not activate , if False this mean i activate and skip .
    ##i apply this mean no activation happen due to True batch norm.
    layer.append(tf.keras.layers.BatchNormalization()) #make batch norm
    #as he may use batch norm and also none for activation , check if we apply activation or not .
    if activation is not None :
      layer.append(tf.keras.layers.Activation(activation))

  return  tf.keras.models.Sequential(layer)

if __name__ =="__main__":
  import tensorflow as tf
  block=encoder_block(32,(3,3),True,'relu')
  print(f"we have now {len(block.layers)} layer(s)")
  print(block.layers)

print("--------------------------")

if __name__ =="__main__":
  import tensorflow as tf
  block=decoder_block(32,(3,3),True,'relu')
  print(f"we have now {len(block.layers)} layer(s)")
  print(block.layers)

# Class to define custom Layer define  AutoEncoder use block Functions
class AutoEncoder(tf.keras.layers.Layer):
  def __init__(self , filters=[16,32,64] , kernel_size=(3,3) ,
                 batch_norm=False , activation="relu" , final_activation=None , out_channels=1):
    tf.keras.layers.Layer.__init__(self)

    assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1 ,"invalied kernal size use (3,3),(5,5) or (7,7) and so on !" # Only odd sizes allowed
    assert len(filters) > 0 ,"You must specify at least one channel."

    #note : no need to self.variables as i will not use outside the def __init__ .

    self.encoder_layers=[]  #must use self.variable to define inside __init__ .
    self.decoder_layers=[]

    for i in range(len(filters)):
      self.encoder_layers.append(encoder_block(filters[i],kernel_size,batch_norm,activation)) #get input_channels-16-32-64
    for i in reversed(range(len(filters)) ):
      self.decoder_layers.append(decoder_block(filters[i],kernel_size,batch_norm,activation)) #get 64-32-16

    # so we have now input_channels-16-32-64 then 64 - 32 -16
    # so we still need out_channels

    self.decoder_layers.append(decoder_block(out_channels,kernel_size,False,final_activation))

    self.encoder=tf.keras.models.Sequential(self.encoder_layers)
    self.decoder=tf.keras.models.Sequential(self.decoder_layers)

  def My_layers_is(self):
    print("Encoder layers:")
    for i, block in enumerate(self.encoder.layers):
        print(f"  Block {i+1}:")
        for layer in block.layers:
            layer_type = layer.__class__.__name__
            print(f"    - {layer_type}")

    print("Decoder layers:")
    for i, block in enumerate(self.decoder.layers):
        print(f"  Block {i+1}:")
        for layer in block.layers:
            layer_type = layer.__class__.__name__
            print(f"    - {layer_type}")

  def call(self , Inputs):
    out=self.encoder(Inputs)
    out=self.decoder(out)

    return out

if __name__ == "__main__":
  import tensorflow as tf
  model = AutoEncoder(batch_norm=True)
  model.My_layers_is()

