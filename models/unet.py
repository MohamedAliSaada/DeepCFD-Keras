# -*- coding: utf-8 -*-
"""UNet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FvaiBdMzLRCR4AhlyrzTtls6ePoegms
"""

import numpy as np
import tensorflow as tf

# the Function to make Encoder blocks
def Encoder(out_channels, kernel_size=(3,3), activation='relu', pool_mod="max", batch_norm=True):
    assert pool_mod in ['max', 'avg'], "invalid use 'max' or 'avg' only"
    assert (kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1), "invalid kernel size!"

    block = []

    # First Conv
    layer = tf.keras.layers.Conv2D(
        out_channels,
        kernel_size,
        padding='same',
        activation=None if batch_norm else activation
    )
    block.append(layer)
    if batch_norm:
        block.append(tf.keras.layers.BatchNormalization())
        block.append(tf.keras.layers.Activation(activation))

    # Second Conv
    layer = tf.keras.layers.Conv2D(
        out_channels,
        kernel_size,
        padding='same',
        activation=None if batch_norm else activation
    )
    block.append(layer)
    if batch_norm:
        block.append(tf.keras.layers.BatchNormalization())
        block.append(tf.keras.layers.Activation(activation))

    # Pooling
    if pool_mod == "max":
        block.append(tf.keras.layers.MaxPooling2D(2, 2))
    else:
        block.append(tf.keras.layers.AveragePooling2D(2, 2))

    # Determine shortcut index
    n = len(block) // 2
    if n == 3:
        shortcut_idx = 5
    elif n == 1:
        shortcut_idx = 1

    return block, shortcut_idx

if __name__ == "__main__":
  import tensorflow as tf
  b,n=Encoder(10,batch_norm=True)
  for i in b:
    print(i)
  print(f"\n shortcut_idx  {n}")

# Decoder block
def Decoder(out_channels, kernel_size=(3, 3), activation='relu', batch_norm=True):
    block = []

    for _ in range(2):  # 2 Conv layers
        layer = tf.keras.layers.Conv2D(
            out_channels,
            kernel_size,
            padding='same',
            activation=None if batch_norm else activation
        )
        block.append(layer)
        if batch_norm:
            block.append(tf.keras.layers.BatchNormalization())
            block.append(tf.keras.layers.Activation(activation))

    return block

if __name__ == "__main__":
  import tensorflow as tf
  b=Decoder(10,batch_norm=False)
  for i in b:
    print(i)

# UNet class
class UNet(tf.keras.layers.Layer):
    def __init__(self, kernel_size=(3, 3), activation='relu', pool_mod="max", batch_norm=True,
                 final_activation=None, filters=[16, 32, 64], out_channels=3):
        super().__init__()

        self.filters = filters
        self.kernel_size = kernel_size
        self.activation = activation
        self.batch_norm = batch_norm

        self.encoder_layers = []
        self.neck_layers = []
        self.decoder_layers = []

        # Build encoder
        for i in range(len(filters)):
            block, _ = Encoder(filters[i], kernel_size, activation, pool_mod, batch_norm)
            self.encoder_layers.append(block)

        self.encoder = tf.keras.models.Sequential([layer for block in self.encoder_layers for layer in block])

        # Neck (without pooling)
        neck_block, _ = Encoder(filters[-1], kernel_size, activation, pool_mod, batch_norm)
        if batch_norm:
            self.neck = tf.keras.models.Sequential(neck_block[:6])
        else:
            self.neck = tf.keras.models.Sequential(neck_block[:2])

        # Decoder (prebuilt here)
        for i in reversed(range(len(filters))):
            upsample = tf.keras.layers.Conv2DTranspose(filters[i], (2, 2), strides=2, padding='same')
            concat = tf.keras.layers.Concatenate()
            block = Decoder(filters[i], kernel_size, activation, batch_norm)
            self.decoder_layers.append((upsample, concat, block))

        # Final output layer
        self.output_conv = tf.keras.layers.Conv2D(out_channels, (1, 1), padding='same',
                                                  activation=final_activation)

    def call(self, inputs):
        skip_layers = []
        out = inputs

        # Run encoder
        for i, layer in enumerate(self.encoder.layers):
            out = layer(out)
            if isinstance(layer, tf.keras.layers.MaxPooling2D) or isinstance(layer, tf.keras.layers.AveragePooling2D):
                skip_layers.append(out)

        # Neck
        out = self.neck(out)

        # Run decoder
        for i, (upsample, concat, block) in enumerate(self.decoder_layers):
            out = upsample(out)
            skip = skip_layers.pop()
            out = concat([out, skip])
            for layer in block:
                out = layer(out)

        # Output
        return self.output_conv(out)

    def My_layers_is(self):
        print("\nEncoder Layers:")
        for layer in self.encoder.layers:
            print("   ", layer)
        print("\nNeck Layers:")
        for layer in self.neck.layers:
            print("   ", layer)
        print("\nDecoder Layers:")
        for i, (up, concat, block) in enumerate(self.decoder_layers):
            print(f"  Decoder Block {i + 1}:")
            print("    Upsample:", up)
            print("    Concat:", concat)
            for l in block:
                print("    ", l)
        print("\nFinal Output Layer:")
        print("   ", self.output_conv)

if __name__ == "__main__":
  import tensorflow as tf
  n=UNet(batch_norm=True ,filters=[8,16])
  n.My_layers_is()

