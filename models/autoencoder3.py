# -*- coding: utf-8 -*-
"""AutoEncoder3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FvaiBdMzLRCR4AhlyrzTtls6ePoegms
"""

import numpy as np
import tensorflow as tf

# Function to define encoder block in my AutoEncoder
def encoder_block( out_channels , kernel_size=(3,3) ,
                 batch_norm=False , activation="relu"):
  assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1 ,"invalied kernal size use (3,3),(5,5) or (7,7) and so on !" # Only odd sizes allowed
  layer=[]
  conv = tf.keras.layers.Conv2D(
    out_channels,kernel_size , padding='same' ,
    activation=None if batch_norm else activation)
  #so i have conv layer now activated if no batch norm and not activated if have batch norm
  layer.append(conv)

  if batch_norm : #if this True this mean i'm not activate , if False this mean i activate and skip .
    ##i apply this mean no activation happen due to True batch norm.
    layer.append(tf.keras.layers.BatchNormalization()) #make batch norm
    #as he may use batch norm and also none for activation , check if we apply activation or not .
    if activation is not None :
      layer.append(tf.keras.layers.Activation(activation))

  return  tf.keras.models.Sequential(layer)

# Function to define decoder block in my AutoEncoder
def decoder_block( out_channels , kernel_size=(3,3) ,
                 batch_norm=False , activation="relu"):
  assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1 ,"invalied kernal size use (3,3),(5,5) or (7,7) and so on !" # Only odd sizes allowed
  layer=[]
  conv = tf.keras.layers.Conv2DTranspose(
    out_channels,kernel_size , padding='same' ,
    activation=None if batch_norm else activation)
  #so i have conv layer now activated if no batch norm and not activated if have batch norm
  layer.append(conv)

  if batch_norm : #if this True this mean i'm not activate , if False this mean i activate and skip .
    ##i apply this mean no activation happen due to True batch norm.
    layer.append(tf.keras.layers.BatchNormalization()) #make batch norm
    #as he may use batch norm and also none for activation , check if we apply activation or not .
    if activation is not None :
      layer.append(tf.keras.layers.Activation(activation))

  return  tf.keras.models.Sequential(layer)

if __name__ =="__main__":
  import tensorflow as tf
  block=encoder_block(32,(3,3),True,'relu')
  print(f"we have now {len(block.layers)} layer(s)")
  print(block.layers)

print("--------------------------")

if __name__ =="__main__":
  import tensorflow as tf
  block=decoder_block(32,(3,3),True,'relu')
  print(f"we have now {len(block.layers)} layer(s)")
  print(block.layers)

import tensorflow as tf

# Class to define custom Layer define  AutoEncoder use block Functions
class AutoEncoder3(tf.keras.layers.Layer):
  def __init__(self , filters=[16,32,64] , kernel_size=(3,3) ,
                 batch_norm=False , activation="relu" , final_activation=None , out_channels=3):
    tf.keras.layers.Layer.__init__(self)

    assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1 ,"invalied kernal size use (3,3),(5,5) or (7,7) and so on !" # Only odd sizes allowed
    assert len(filters) > 0 ,"You must specify at least one channel."

    #note : no need to self.variables as i will not use outside the def __init__ .

    self.encoder_layers=[]  #must use self.variable to define inside __init__ .
    self.decoder_layers=[[] for _ in range(out_channels)] #this  mean have n path equal out_channels

    for i in range(len(filters)):
      self.encoder_layers.append(encoder_block(filters[i],kernel_size,batch_norm,activation)) #get input_channels-16-32-64

    # so we have now input_channels-16-32-64 and need now use it in n paths equal out_channels

            ################## out_channels-1
            #
    ########################## out_channels-2
            #
            ################## out_channels-3 and so on

    #we End with encoder
    self.encoder=tf.keras.models.Sequential(self.encoder_layers)

    for i in range(out_channels):  #let's say 0,1,2 for 3 channels
      #we need to make for each channel decoder
      for z in reversed(range(len(filters))): #reverse to be 64,32,16
        self.decoder_layers[i].append(decoder_block(filters[z],kernel_size,batch_norm,activation))

      self.decoder_layers[i].append(decoder_block(1,kernel_size,False,final_activation)) #this become 1 channel output

    # we End with decoders
    self.decoders=[tf.keras.models.Sequential(path) for path in self.decoder_layers ] # now have 3 paths or sequential models.

  def My_layers_is(self):
    print("Encoder layers:")
    for i, block in enumerate(self.encoder.layers):
        print(f"  Block {i+1}:")
        for layer in block.layers:
            layer_type = layer.__class__.__name__
            print(f"    - {layer_type}")

    print("Decoder layers:")
    for i,paths in enumerate(self.decoders):
      print(f"  paths {i+1}:")
      for i,blocks in enumerate(paths.layers):
        print(f"  blocks {i+1}:")
        for layers in blocks.layers:
          layer_type = layers.__class__.__name__
          print(f"    - {layer_type}")



  def call(self , Inputs):
    out=self.encoder(Inputs)
    out = [path(out) for path in self.decoders]
    out=tf.concat(out , axis=-1)
    return out

if __name__ == "__main__":
  import tensorflow as tf
  model = AutoEncoder3(batch_norm=False)
  model.My_layers_is()

