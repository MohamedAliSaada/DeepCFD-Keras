# -*- coding: utf-8 -*-
"""UNet3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FvaiBdMzLRCR4AhlyrzTtls6ePoegms
"""

import numpy as np
import tensorflow as tf

# the Function to make Encoder blocks
def Encoder(out_channels, kernel_size=(3,3), activation='relu', pool_mod="max", batch_norm=True):
    assert pool_mod in ['max', 'avg'], "invalid use 'max' or 'avg' only"
    assert (kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1), "invalid kernel size!"

    block = []

    # First Conv
    layer = tf.keras.layers.Conv2D(
        out_channels,
        kernel_size,
        padding='same',
        activation=None if batch_norm else activation
    )
    block.append(layer)
    if batch_norm:
        block.append(tf.keras.layers.BatchNormalization())
        block.append(tf.keras.layers.Activation(activation))

    # Second Conv
    layer = tf.keras.layers.Conv2D(
        out_channels,
        kernel_size,
        padding='same',
        activation=None if batch_norm else activation
    )
    block.append(layer)
    if batch_norm:
        block.append(tf.keras.layers.BatchNormalization())
        block.append(tf.keras.layers.Activation(activation))

    # Pooling
    if pool_mod == "max":
        block.append(tf.keras.layers.MaxPooling2D(2, 2))
    else:
        block.append(tf.keras.layers.AveragePooling2D(2, 2))

    return block

# check functinality
if __name__ == "__main__":
  import tensorflow as tf
  b=Encoder(10,batch_norm=False)
  for i in b:
    if isinstance(i, tf.keras.layers.MaxPooling2D):
      break
    if isinstance(i, tf.keras.layers.AveragePooling2D):
      break

    print(f'{i}')

# how to get layer befor pooling is applied
if __name__ == "__main__":
  for n,i in enumerate(b):
    if isinstance(i,tf.keras.layers.MaxPooling2D):
      print(f'\n the shortcut_id is :{n-1}')

# SubDecoder block
def SubDecoder(out_channels, kernel_size=(3, 3), activation='relu', batch_norm=True):

    block = []

    for _ in range(2):  # 2 Conv layers
        layer = tf.keras.layers.Conv2D(
            out_channels,
            kernel_size,
            padding='same',
            activation=None if batch_norm else activation
        )
        block.append(layer)
        if batch_norm:
            block.append(tf.keras.layers.BatchNormalization())
            block.append(tf.keras.layers.Activation(activation))

    return block

# check functinality
if __name__ == "__main__":
  import tensorflow as tf
  b=SubDecoder(10,batch_norm=True)
  for i in b:
    print(i)

# UNet class
class UNet3(tf.keras.layers.Layer):
    def __init__(self, kernel_size=(3, 3), activation='relu', pool_mod="max", batch_norm=True,
                 final_activation=None, filters=[16, 32, 64], out_channels=3):
        super().__init__()

        self.filters = filters
        self.kernel_size = kernel_size
        self.activation = activation
        self.batch_norm = batch_norm

        ##################################
        # My three basic steps              #--> done
        ##################################
        self.encoder_layers = []      #--> done
        self.neck_layers    = []      #--> done
        self.path_decoder_layers = [[] for _ in range(out_channels)]      #--> done

        #################
        # Build encoder    #--> done
        #################
        for i in range(len(filters)):
            self.encoder_layers.append(Encoder(filters[i], kernel_size, activation, pool_mod, batch_norm))
            # this mean i have now 16-32-64 layers of conv2d applied to input
            # but each append have interior layers

        self.encoder = tf.keras.models.Sequential([layer for block in self.encoder_layers for layer in block ])


        ##################################
        # Neck (Encoder without pooling)     #--> done
        ##################################
        neck_block = Encoder(filters[-1], kernel_size, activation, pool_mod, batch_norm)
        for layer in  neck_block:
          if isinstance(layer, tf.keras.layers.MaxPooling2D) or isinstance(layer, tf.keras.layers.AveragePooling2D) :
            break
          else:
            self.neck_layers.append(layer)

        self.neck = tf.keras.models.Sequential(self.neck_layers)


        ##############
        # Decoder      #--> done
        ##############
        # you should now that , you in this stage build layers of decoder , not decoder it self
        # so we need sequnce of it's layers only .
        # so we need Conv2DTranspose layer then Concatenate layer then SubDecoder , and make this filter len times

        """
        we now need n paths depend on out chanels we need to have , and each end with output layer of it's own
        so :iterate over paths and
        1-make all needed layers as it done for one . but save in i of encoder layers holder
        2-note to end each with output layer that have 1 channel
        3-after end have for example 3 list each contain it's own layers
        4-make each list layers as sequential model

        """
        self.paths_len = out_channels
        for p in range(self.paths_len):
          for i in reversed(range(len(filters))) :
            self.path_decoder_layers[p].append(tf.keras.layers.Conv2DTranspose(filters[i], (2, 2), strides=2, padding='same'))
            self.path_decoder_layers[p].append(tf.keras.layers.Concatenate())

            # SubDecoder contain list of layers , so need to loop
            end_decoder_block = SubDecoder(filters[i], kernel_size, activation, batch_norm)
            for layer in end_decoder_block :
              self.path_decoder_layers[p].append(layer)

          #add out layer with 1 channle
          self.path_decoder_layers[p].append(tf.keras.layers.Conv2D(1, (1, 1), padding='same',activation=final_activation))

        #make these layers in p indx as sequntail model
        self.decoder= [tf.keras.models.Sequential(layers) for layers in self.path_decoder_layers ]



    def My_layers_is(self):         #--> done
        print("\nEncoder Layers:")
        for layer in self.encoder.layers:
            print("   ", layer)

        print("\nNeck Layers:")
        for layer in self.neck.layers:
            print("   ", layer)

        print("\nDecoder Layers:")
        for path in range(self.paths_len):
            print(f"\n the path-{path} layers is \n ")
            for layer in self.decoder[path].layers:
              print("   ", layer)


    def call(self, inputs):
      # you need to apply to encoder layers one after another , but you need to save also ,
      # the data that you will use at decoder as skip connections to match u net style .
      # In classic U-Net, the output to be concatenated is after the last activation before each MaxPooling2D or even AveragePooling.
        skip_layers = []
        skip_id =[]

        ################################
        # first make out equal input      #--> done
        ################################
        out = inputs

        ################
        # Run encoder      #--> done
        ################
        """
        what common in three paths is skip_layers:

          if layer is pool , so we need to keep output we have before apply it .
          so we must know pool  id in ower layers first.

          then loop and apply layers with two cases
          1-if i==id apply and save this mean i apply layer befor pool and save
          2-if i!= id apply only   this mean i apply all layers except befor pool and save only .
        """
        for i , layer in enumerate(self.encoder.layers):
          if isinstance(layer, tf.keras.layers.MaxPooling2D) or isinstance(layer, tf.keras.layers.AveragePooling2D):
            skip_id.append(i-1)

        for i , layer in enumerate(self.encoder.layers):
          if i in skip_id:
            out = layer(out)
            skip_layers.append(out)
          else :
            out = layer(out)

        ############
        # Run neck    #--> done
        ############
        out = self.neck(out)

        ###############
        # Run decoder      #--> done
        ################
        """
        i have now 3 paths :

          i now have decoder layers for each path , want to apply by this logic
          1-when have transpose conv  out = layer(out)
          2- when have concat make    out = layer(out + skip_layers)
          but use skip layers from last to first one , so you can use pop for that .
          3- when have other make rugular as out=layer(out)

          so we need to know exactly when is concat happen to apply and else make regualr thing

        """
        # use any path to get id_concat , and it same for all three of them
        id_concat =[]
        for i,layer in enumerate(self.path_decoder_layers[0]):
          if isinstance(layer, tf.keras.layers.Concatenate):
            id_concat.append(i)

        '''
        i have now [model, model , model]
        so need to get out as [out1 , out2 , out3 ] by pass out to each of them
        it's like out=[out1 , out2 , out3]
        but model itself is layers

        so loop and say
        1-go n times each time use model[n] if them
        2-apply using this model[n] layers
        3-safe output at list then concatenate

        '''
        results = []
        old_out = out
        for path in range(self.paths_len):
          out_path = old_out  # start with the neck output for this path
          skip_path = skip_layers.copy()  # create a copy to pop from
          for i,layer in enumerate(self.decoder[path].layers):
            if i in id_concat:
              out_path = layer([out_path, skip_path.pop()])
            else:
              out_path = layer(out_path)

          results.append(out_path)


        #############
        # concat outputs
        #############
        out = tf.concat(results , axis =-1)

        return out

# check code functinality
if __name__ == "__main__":
  import tensorflow as tf
  n=UNet3(batch_norm=False)
  n.My_layers_is()

